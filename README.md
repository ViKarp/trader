# RL Trader

Проект предоставляет модульную платформу для исследования стратегий алгоритмической торговли на базе обучения с подкреплением. В основе лежит гибкая среда `MultiAssetTradingEnv`, имитирующая торговлю несколькими инструментами, и реализация агента PPO для обучения на синтетических или реальных данных.

## Структура репозитория

```
trader_rl/
├── agents/           # Алгоритмы обучения (PPO и будущие агенты)
├── config/           # Конфигурации среды и алгоритмов
├── data/             # Генераторы и загрузчики данных
├── envs/             # Реализации торговых сред Gymnasium
├── features/         # Утилиты для построения технических признаков
├── models/           # Нейронные сети (политика, энкодеры и т.д.)
├── scripts/          # Исполняемые сценарии (демо-обучение, эксперименты)
└── __init__.py       # Упрощённый доступ к ключевым компонентам пакета
```

Такое разделение облегчает добавление новых агентов, моделей, индикаторов и сценариев обучения без изменения существующей логики.

## Быстрый старт

1. Установите зависимости (PyTorch, Gymnasium, pandas, numpy).
2. Запустите демонстрационный сценарий обучения:

   ```bash
   python -m trader_rl.scripts.train_demo
   ```

   Скрипт создаст (или переиспользует) файл `market_sample.csv` с синтетическими данными, соберёт среду и обучит PPO-агента несколько итераций. По завершении будет сформирована папка `artifacts/<timestamp>_ppo_demo` с метриками, графиками и сохранёнными весами модели (`policy.pt`).

3. Для обратной совместимости можно также выполнить:

   ```bash
   python first_entry.py
   ```

   Этот скрипт перенаправляет выполнение к демонстрационному сценарию.

## Расширение

- **Новые данные:** добавляйте функции загрузки в `trader_rl/data` и используйте их в собственных скриптах.
- **Новые признаки:** реализуйте индикаторы в `trader_rl/features` и подключайте их внутри среды или моделей.
- **Новые модели:** размещайте нейронные архитектуры в `trader_rl/models` и передавайте их агентам.
- **Новые агенты:** создавайте модули в `trader_rl/agents` для альтернативных алгоритмов (SAC, DDPG, IQL и т.п.).

## Логирование экспериментов

Сценарий `train_demo` теперь поддерживает управление экспериментами через аргументы командной строки:

```bash
python -m trader_rl.scripts.train_demo \
    --total-updates 50 \
    --log-dir runs \
    --run-name stress_test \
    --weights checkpoints/policy.pt
```

- Все обновления обучения логируются в консоль и сохраняются в CSV/JSON внутри каталога `runs/<timestamp>_stress_test`.
- После завершения формируется график `training_dynamics.png`, а также весовые коэффициенты модели `policy.pt` с проверкой совместимости архитектуры.
- Параметр `--weights` позволяет выбрать стартовый чекпойнт. Если архитектура модели не совпадает с сохранёнными весами, выполнение прерывается с понятным сообщением.

## Предупреждение

Код предназначен исключительно для исследований и обучения. Он не является инвестиционной рекомендацией и не гарантирует финансовых результатов.