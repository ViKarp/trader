# RL Trader

Проект предоставляет модульную платформу для исследования стратегий алгоритмической торговли на базе обучения с подкреплением. В основе лежит гибкая среда `MultiAssetTradingEnv`, имитирующая торговлю несколькими инструментами, и реализация агента PPO для обучения на синтетических или реальных данных.

## Структура репозитория

```
trader_rl/
├── agents/           # Алгоритмы обучения (PPO и будущие агенты)
├── config/           # Конфигурации среды и алгоритмов
├── data/             # Генераторы и загрузчики данных
├── envs/             # Реализации торговых сред Gymnasium
├── features/         # Утилиты для построения технических признаков
├── models/           # Нейронные сети (политика, энкодеры и т.д.)
├── scripts/          # Исполняемые сценарии (демо-обучение, эксперименты)
└── __init__.py       # Упрощённый доступ к ключевым компонентам пакета
```

Такое разделение облегчает добавление новых агентов, моделей, индикаторов и сценариев обучения без изменения существующей логики.

## Быстрый старт

1. Установите зависимости (PyTorch, Gymnasium, pandas, numpy, matplotlib).
2. Запустите демонстрационный сценарий обучения:

   ```bash
   python -m trader_rl.scripts.train_demo
   ```

   Скрипт создаст (или переиспользует) файл `market_sample.csv` с синтетическими данными, соберёт среду и обучит PPO-агента несколько итераций. Во время обучения в консоли будут отображаться ключевые метрики обновлений, а все артефакты сохранятся в директории `experiments/<timestamp>`.

   Основные параметры можно переопределить:

   ```bash
   python -m trader_rl.scripts.train_demo \
       --total-updates 50 \
       --experiment-name test_run \
       --initial-weights path/to/checkpoint.pt
   ```

   Флаг `--overwrite` разрешает использовать существующую папку эксперимента повторно.

3. Чтобы обучить агента на исторических данных US equities, скачайте и распакуйте датасет так, чтобы структура выглядела как `Data/Stocks/*.txt` и `Data/ETFs/*.txt`. Затем укажите путь к каталогу, содержащему папку `Data`:

   ```bash
   python -m trader_rl.scripts.train_us_equities /path/to/dataset/root \
       --symbols AAPL MSFT GOOGL \
       --asset-folder Stocks \
       --total-updates 300
   ```

   Скрипт автоматически создаст среду `HistoricalEquityTradingEnv`, выполнит ресемплинг и обработку пропусков и сохранит артефакты обучения в `experiments/<timestamp>`. При отсутствии аргумента `--symbols` будут использованы все доступные тикеры в выбранной папке.

4. Для обратной совместимости можно также выполнить:

   ```bash
   python first_entry.py
   ```

   Этот скрипт перенаправляет выполнение к демонстрационному сценарию.

## Расширение

- **Новые данные:** добавляйте функции загрузки в `trader_rl/data` и используйте их в собственных скриптах.
- **Новые признаки:** реализуйте индикаторы в `trader_rl/features` и подключайте их внутри среды или моделей.
- **Новые модели:** размещайте нейронные архитектуры в `trader_rl/models` и передавайте их агентам.
- **Новые агенты:** создавайте модули в `trader_rl/agents` для альтернативных алгоритмов (SAC, DDPG, IQL и т.п.).

## Логирование экспериментов

Класс `ExperimentLogger` управляет папкой эксперимента: сохраняет конфигурацию (`config.json`), метрики обучения (`metrics.csv`), график изменения капитала (`equity.png`), произвольные метаданные и финальные веса модели (`model_weights.pt`).

Агент `PPOAgent` умеет загружать стартовые веса (`--initial-weights` в демо-скрипте) и проверяет, что структура сохранённой модели совпадает с текущей архитектурой. При несовпадении ключей или размеров тензоров выбрасывается осмысленная ошибка.

## Предупреждение

Код предназначен исключительно для исследований и обучения. Он не является инвестиционной рекомендацией и не гарантирует финансовых результатов.